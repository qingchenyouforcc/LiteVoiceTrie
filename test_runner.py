#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘½ä»¤ Trie åŒ¹é…å™¨çš„è‡ªåŠ¨æµ‹è¯•è¿è¡Œå™¨ã€‚

å®ƒè¯„ä¼°ï¼š
1) ç²¾ç¡®åŒ¹é…
2) Trie.match_fuzzy_sub1 å®ç°çš„æ¨¡ç³ŠåŒ¹é…ï¼ˆå•å­—ç¬¦æ›¿æ¢ï¼‰

è¾“å…¥æ•°æ®é›†æ ¼å¼ (TSV, UTF-8)ï¼š
<asr_æ–‡æœ¬>\t<æœŸæœ›_å‘½ä»¤>

ç¤ºä¾‹ï¼š
å…³é—­ç©ºè°ƒå•Š\tå…³é—­ç©ºè°ƒ
"""

from __future__ import annotations

import argparse
import statistics
import time
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Tuple

from tool import load_commands_from_file, normalize_asr, strip_tail_particle
from voice_trie import Trie


@dataclass
class CaseResult:
    raw: str
    expected: str
    normalized: str
    predicted: Optional[str]
    mode: str
    latency_ms: float
    ok: bool


def build_trie(commands_file: str, enable_length_check: bool = True) -> Tuple[Trie, Dict[str, str]]:
    """
    å…³é”®ä¿®å¤ï¼šæ„å»º Trie æ—¶ä¹Ÿå¯¹å‘½ä»¤è¯å…¸åšåŒæ ·çš„ normalize_asrï¼Œä¿è¯â€œè¾“å…¥ä¾§â€å’Œâ€œè¯å…¸ä¾§â€å¤„åœ¨åŒä¸€ç©ºé—´ã€‚
    åŒæ—¶è¿”å› norm2raw æ˜ å°„ï¼Œç”¨äºæŠŠåŒ¹é…åˆ°çš„å½’ä¸€åŒ–å‘½ä»¤è¿˜åŸæˆåŸå§‹å‘½ä»¤ï¼ˆä¸æµ‹è¯•é›† expected å¯¹é½ï¼‰ã€‚
    """
    commands = load_commands_from_file(commands_file)

    trie = Trie()
    norm2raw: Dict[str, str] = {}
    norm_cmds: List[str] = []

    for cmd in commands:
        key = normalize_asr(cmd)
        norm_cmds.append(key)

        try:
            trie.insert(key, cmd)
        except TypeError:
            trie.insert(key)

        # è‹¥å¤šä¸ªåŸå§‹å‘½ä»¤å½’ä¸€åŒ–åå†²çªï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆä½ ä¹Ÿå¯ä»¥æ”¹æˆæ‰“å° warningï¼‰
        norm2raw.setdefault(key, cmd)

    # é•¿åº¦å‰ªæä¹Ÿè¦ç”¨å½’ä¸€åŒ–åçš„é•¿åº¦é›†åˆï¼Œå¦åˆ™ä¼šè¯¯å‰ªæ‰ï¼ˆå¦‚ â€œå…³é—­ç©ºè°ƒâ€(4) -> â€œå…³ç©ºè°ƒâ€(3)ï¼‰
    trie.set_vaild_lens(norm_cmds)
    trie.set_length_check(enable_length_check)
    return trie, norm2raw


def predict(trie: Trie, norm2raw: Dict[str, str], raw: str) -> Tuple[str, Optional[str], float, str]:
    """è¿”å› (æ¨¡å¼, é¢„æµ‹ç»“æœ, å»¶è¿Ÿ_ms, å½’ä¸€åŒ–åçš„æ–‡æœ¬)ã€‚"""
    s = strip_tail_particle(raw.strip())
    s = normalize_asr(s)

    start = time.perf_counter()
    ans = trie.match_exact(s)
    mode = "exact"
    if not ans:
        ans = trie.match_fuzzy_sub1(s)
        mode = "fuzzy_sub1" if ans else "unrecognized"
    elapsed_ms = (time.perf_counter() - start) * 1000.0

    # æŠŠå½’ä¸€åŒ–çš„åŒ¹é…ç»“æœæ˜ å°„å›åŸå§‹å‘½ä»¤ï¼ˆä¸ TSV ä¸­ expected ä¿æŒä¸€è‡´ï¼‰
    pred = norm2raw.get(ans, ans) if ans else None
    return mode, pred, elapsed_ms, s


def iter_dataset(dataset_path: str) -> Iterable[Tuple[str, str]]:
    with open(dataset_path, "r", encoding="utf-8") as f:
        for line_no, line in enumerate(f, start=1):
            line = line.rstrip("\n")
            if not line.strip():
                continue
            parts = line.split("\t")
            if len(parts) != 2:
                raise ValueError(
                    f"{dataset_path}:{line_no}: é¢„æœŸæœ‰ 2 åˆ— TSV æ•°æ® (è¾“å…¥\\tæœŸæœ›å€¼)ï¼Œä½†å¾—åˆ° {len(parts)} åˆ—: {line!r}"
                )
            yield parts[0], parts[1]


def evaluate(trie: Trie, norm2raw: Dict[str, str], dataset_path: str) -> List[CaseResult]:
    results: List[CaseResult] = []
    for raw, expected in iter_dataset(dataset_path):
        mode, pred, latency_ms, normalized = predict(trie, norm2raw, raw)
        ok = (pred == expected)
        results.append(
            CaseResult(
                raw=raw,
                expected=expected,
                normalized=normalized,
                predicted=pred,
                mode=mode,
                latency_ms=latency_ms,
                ok=ok,
            )
        )
    return results


def summarize(results: List[CaseResult], title: str, show_failures: int = 20) -> None:
    total = len(results)
    correct = sum(1 for r in results if r.ok)
    acc = (correct / total * 100.0) if total else 0.0

    latencies = [r.latency_ms for r in results]
    p50 = statistics.median(latencies) if latencies else 0.0
    p95 = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else (max(latencies) if latencies else 0.0)
    mx = max(latencies) if latencies else 0.0

    mode_counts = {}
    for r in results:
        mode_counts[r.mode] = mode_counts.get(r.mode, 0) + 1

    print(f"\n=== {title} ===")
    print(f"ç”¨ä¾‹æ€»æ•°: {total}")
    print(f"åŒ¹é…æ­£ç¡®: {correct}")
    print(f"å‡†ç¡®ç‡: {acc:.2f}%")
    print(f"è€—æ—¶ (ms): p50={p50:.3f}, p95={p95:.3f}, æœ€å¤§å€¼={mx:.3f}")
    print("åŒ¹é…æ¨¡å¼ç»Ÿè®¡:", ", ".join(f"{k}={v}" for k, v in sorted(mode_counts.items())))

    failures = [r for r in results if not r.ok]
    if failures:
        print(f"\nå‰ {min(show_failures, len(failures))} ä¸ªå¤±è´¥ç”¨ä¾‹:")
        for r in failures[:show_failures]:
            print(
                f"- åŸå§‹={r.raw!r} -> å½’ä¸€åŒ–={r.normalized!r} | æœŸæœ›={r.expected!r} | é¢„æµ‹={r.predicted!r} | æ¨¡å¼={r.mode} | {r.latency_ms:.3f}ms"
            )
    else:
        print("\næ— å¤±è´¥ç”¨ä¾‹ ğŸ‰")


def main() -> None:
    ap = argparse.ArgumentParser(description="è¯­éŸ³å‘½ä»¤ Trie åŒ¹é…å™¨è‡ªåŠ¨æµ‹è¯•ç¨‹åºã€‚")
    ap.add_argument(
        "--commands",
        default="commands.txt",
        help="å‘½ä»¤è¯å…¸æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªå‘½ä»¤ï¼‰ã€‚é»˜è®¤å€¼ï¼šcommands.txt",
    )
    ap.add_argument(
        "--datasets",
        nargs="+",
        default=[
            "test_dataset/dataset_extra_char.tsv",
            "test_dataset/dataset_extra_particle.tsv",
            "test_dataset/dataset_extra_char_and_particle.tsv",
        ],
        help="ä¸€ä¸ªæˆ–å¤šä¸ª TSV æ•°æ®é›†ã€‚é»˜è®¤å€¼ï¼šå†…ç½®çš„ 3 ä¸ªæ•°æ®é›†ã€‚",
    )
    ap.add_argument(
        "--no-length-check",
        action="store_true",
        help="ç¦ç”¨ Trie é•¿åº¦å‰ªæ (Trie.set_length_check(False))ã€‚",
    )
    ap.add_argument(
        "--show-failures",
        type=int,
        default=20,
        help="æ¯ä¸ªæ•°æ®é›†æ‰“å°å¤šå°‘ä¸ªå¤±è´¥ç”¨ä¾‹ã€‚é»˜è®¤å€¼ï¼š20",
    )
    args = ap.parse_args()

    trie, norm2raw = build_trie(args.commands, enable_length_check=(not args.no_length_check))

    all_results: List[CaseResult] = []
    for ds in args.datasets:
        res = evaluate(trie, norm2raw, ds)
        summarize(res, title=ds, show_failures=args.show_failures)
        all_results.extend(res)

    # Overall summary (across all datasets)
    if len(args.datasets) > 1:
        summarize(all_results, title="æ€»è®¡", show_failures=args.show_failures)


if __name__ == "__main__":
    main()
    input("\nDone. Press Enter to exit...")